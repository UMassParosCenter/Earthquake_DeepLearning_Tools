{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee0b87-f55a-4cf5-a7ca-510df9aa60c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQ array shape: (420, 11, 52)\n",
      "BG array shape: (694, 11, 52)\n",
      "Saved mean and std to mean.npy and std.npy\n",
      "Starting Fold 1/5\n",
      "Fold 1, Epoch [1/70] Train Loss: 0.6462, Train Acc: 0.3692 Val Loss: 0.6421, Val Acc: 0.3767\n",
      "Fold 1, Epoch [10/70] Train Loss: 0.2561, Train Acc: 0.9057 Val Loss: 0.3038, Val Acc: 0.9148\n",
      "Fold 1, Epoch [20/70] Train Loss: 0.1811, Train Acc: 0.9259 Val Loss: 0.1858, Val Acc: 0.9417\n",
      "Fold 1, Epoch [30/70] Train Loss: 0.1500, Train Acc: 0.9484 Val Loss: 0.1619, Val Acc: 0.9283\n",
      "Fold 1, Epoch [40/70] Train Loss: 0.1468, Train Acc: 0.9428 Val Loss: 0.1632, Val Acc: 0.9552\n",
      "Fold 1, Epoch [50/70] Train Loss: 0.1396, Train Acc: 0.9439 Val Loss: 0.1425, Val Acc: 0.9507\n",
      "Early stopping triggered.\n",
      "\n",
      "Classification report for fold 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9697    0.9209    0.9446       139\n",
      "           1     0.8791    0.9524    0.9143        84\n",
      "\n",
      "    accuracy                         0.9327       223\n",
      "   macro avg     0.9244    0.9366    0.9295       223\n",
      "weighted avg     0.9356    0.9327    0.9332       223\n",
      "\n",
      "Starting Fold 2/5\n",
      "Fold 2, Epoch [1/70] Train Loss: 0.7405, Train Acc: 0.6319 Val Loss: 0.6950, Val Acc: 0.7085\n",
      "Fold 2, Epoch [10/70] Train Loss: 0.2691, Train Acc: 0.8777 Val Loss: 0.2327, Val Acc: 0.9283\n",
      "Fold 2, Epoch [20/70] Train Loss: 0.1849, Train Acc: 0.9259 Val Loss: 0.1621, Val Acc: 0.9507\n",
      "Fold 2, Epoch [30/70] Train Loss: 0.1538, Train Acc: 0.9394 Val Loss: 0.1470, Val Acc: 0.9417\n",
      "Fold 2, Epoch [40/70] Train Loss: 0.1442, Train Acc: 0.9360 Val Loss: 0.1389, Val Acc: 0.9552\n",
      "Early stopping triggered.\n",
      "\n",
      "Classification report for fold 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9574    0.9712    0.9643       139\n",
      "           1     0.9512    0.9286    0.9398        84\n",
      "\n",
      "    accuracy                         0.9552       223\n",
      "   macro avg     0.9543    0.9499    0.9520       223\n",
      "weighted avg     0.9551    0.9552    0.9550       223\n",
      "\n",
      "Starting Fold 3/5\n",
      "Fold 3, Epoch [1/70] Train Loss: 0.6962, Train Acc: 0.4377 Val Loss: 0.6871, Val Acc: 0.4933\n",
      "Fold 3, Epoch [10/70] Train Loss: 0.2525, Train Acc: 0.9068 Val Loss: 0.2813, Val Acc: 0.9148\n",
      "Fold 3, Epoch [20/70] Train Loss: 0.1734, Train Acc: 0.9439 Val Loss: 0.2058, Val Acc: 0.9103\n",
      "Fold 3, Epoch [30/70] Train Loss: 0.1433, Train Acc: 0.9473 Val Loss: 0.1965, Val Acc: 0.9372\n",
      "Fold 3, Epoch [40/70] Train Loss: 0.1222, Train Acc: 0.9562 Val Loss: 0.1878, Val Acc: 0.9327\n",
      "Early stopping triggered.\n",
      "\n",
      "Classification report for fold 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9375    0.9712    0.9541       139\n",
      "           1     0.9494    0.8929    0.9202        84\n",
      "\n",
      "    accuracy                         0.9417       223\n",
      "   macro avg     0.9434    0.9320    0.9372       223\n",
      "weighted avg     0.9420    0.9417    0.9413       223\n",
      "\n",
      "Starting Fold 4/5\n",
      "Fold 4, Epoch [1/70] Train Loss: 0.7017, Train Acc: 0.3906 Val Loss: 0.6758, Val Acc: 0.3722\n",
      "Fold 4, Epoch [10/70] Train Loss: 0.2620, Train Acc: 0.9035 Val Loss: 0.2502, Val Acc: 0.9417\n",
      "Fold 4, Epoch [20/70] Train Loss: 0.1764, Train Acc: 0.9282 Val Loss: 0.1907, Val Acc: 0.9417\n",
      "Early stopping triggered.\n",
      "\n",
      "Classification report for fold 4:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9315    0.9784    0.9544       139\n",
      "           1     0.9610    0.8810    0.9193        84\n",
      "\n",
      "    accuracy                         0.9417       223\n",
      "   macro avg     0.9463    0.9297    0.9368       223\n",
      "weighted avg     0.9426    0.9417    0.9412       223\n",
      "\n",
      "Starting Fold 5/5\n",
      "Fold 5, Epoch [1/70] Train Loss: 0.6069, Train Acc: 0.3767 Val Loss: 0.6144, Val Acc: 0.3784\n",
      "Fold 5, Epoch [10/70] Train Loss: 0.2412, Train Acc: 0.9058 Val Loss: 0.2274, Val Acc: 0.9459\n",
      "Fold 5, Epoch [20/70] Train Loss: 0.1747, Train Acc: 0.9103 Val Loss: 0.1481, Val Acc: 0.9685\n",
      "Fold 5, Epoch [30/70] Train Loss: 0.1529, Train Acc: 0.9484 Val Loss: 0.1281, Val Acc: 0.9685\n",
      "Fold 5, Epoch [40/70] Train Loss: 0.1253, Train Acc: 0.9540 Val Loss: 0.1282, Val Acc: 0.9595\n",
      "Early stopping triggered.\n",
      "\n",
      "Classification report for fold 5:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9643    0.9783    0.9712       138\n",
      "           1     0.9634    0.9405    0.9518        84\n",
      "\n",
      "    accuracy                         0.9640       222\n",
      "   macro avg     0.9639    0.9594    0.9615       222\n",
      "weighted avg     0.9640    0.9640    0.9639       222\n",
      "\n",
      "\n",
      "===== Final Cross-Validation Metrics =====\n",
      "Accuracy:  0.9470\n",
      "Precision: 0.9392\n",
      "Recall:    0.9190\n",
      "F1 Score:  0.9290\n",
      "ROC AUC:   0.9750\n",
      "Confusion Matrix:\n",
      "[[669  25]\n",
      " [ 34 386]]\n",
      "\n",
      "Cross-validation results over 5 folds:\n",
      "Average Train Accuracy: 0.9450 +/- 0.0079\n",
      "Average Validation Accuracy: 0.9471 +/- 0.0111\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Earthquake vs Background PSD Classification Training Script Using 2D CNN and Stratified K-Fold Cross-Validation\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "This script performs supervised training of a 2D convolutional neural network (CNN) \n",
    "to classify Power Spectral Density (PSD) feature windows as earthquake events or background noise.\n",
    "\n",
    "Main Features:\n",
    "--------------\n",
    "- Loads preprocessed PSD datasets (earthquake and background) saved as pickle files.\n",
    "- Converts PSD data into log-scale and applies per-feature normalization (mean/std).\n",
    "- Defines a PyTorch Dataset and DataLoader for efficient batch processing.\n",
    "- Implements class weighting with bias factor to handle class imbalance.\n",
    "- Trains an EarthquakeCNN2d model using Stratified K-Fold cross-validation for robust evaluation.\n",
    "- Includes early stopping based on validation loss to prevent overfitting.\n",
    "- Logs training and validation loss and accuracy per epoch.\n",
    "- Saves model checkpoints and normalized data splits per fold.\n",
    "- Computes and prints detailed classification metrics and confusion matrix after cross-validation.\n",
    "\n",
    "Dependencies:\n",
    "-------------\n",
    "- PyTorch and torch_optimizer (RAdam optimizer)\n",
    "- NumPy\n",
    "- scikit-learn (for stratified splitting, metrics, and class weights)\n",
    "- Custom utilities: psd_pickle_utils (data loading), cnn_model (model definition)\n",
    "\n",
    "Inputs:\n",
    "-------\n",
    "- PSD pickle files containing labeled earthquake and background PSD windows.\n",
    "- Normalization parameters are computed internally.\n",
    "- Model architecture expects input shape: (windows=11, freq_bins=52).\n",
    "\n",
    "Outputs:\n",
    "--------\n",
    "- Saved normalization statistics (mean.npy, std.npy) for inference normalization.\n",
    "- Model checkpoints saved per fold.\n",
    "- Classification reports printed after each fold and overall metrics after all folds.\n",
    "\n",
    "Author: Ethan Gelfand\n",
    "Date: 08/07/2025\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch_optimizer as optim\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from psd_pickle_utils import load_pickle_data, extract_psd_array\n",
    "from cnn_model import EarthquakeCNN2d\n",
    "\n",
    "# --- PyTorch Dataset ---\n",
    "class PSD_Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # X shape: (events, windows, freq_bins)\n",
    "        # Add channel dim for CNN2d: (events, 1, windows, freq_bins)\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "\n",
    "# --- Early stopping ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            return\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "# --- Load the dataset ---\n",
    "patheq = \"../DataCollection_Preprocessing/Exported_Paros_Data/PSD_Windows_Earthquake_100Hz.pkl\"\n",
    "pathbg = \"../DataCollection_Preprocessing/Exported_Paros_Data/PSD_Windows_Background_100Hz.pkl\"\n",
    "\n",
    "EarthquakeData = load_pickle_data(patheq)\n",
    "BackgroundData = load_pickle_data(pathbg)\n",
    "\n",
    "# convert to 3D numpy arrays: (events, windows, freq_bins)\n",
    "eq_array = extract_psd_array(EarthquakeData)  # (417, 11, 52)\n",
    "bg_array = extract_psd_array(BackgroundData)  # (684, 11, 52)\n",
    "\n",
    "print(\"EQ array shape:\", eq_array.shape)\n",
    "print(\"BG array shape:\", bg_array.shape)\n",
    "\n",
    "# Assign labels\n",
    "eq_labels = np.ones(len(eq_array), dtype=int)\n",
    "bg_labels = np.zeros(len(bg_array), dtype=int)\n",
    "X = np.concatenate([eq_array, bg_array], axis=0)  # shape (events, windows, freq_bins)\n",
    "y = np.concatenate([eq_labels, bg_labels], axis=0)\n",
    "\n",
    "# Shuffle dataset\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# --- Normalization ---\n",
    "X_log = np.log10(X + 1e-10)  # avoid log(0)\n",
    "mean = X_log.mean(axis=0)  # mean per (window, freq_bin)\n",
    "std = X_log.std(axis=0) + 1e-6  # prevent div by zero\n",
    "\n",
    "X = (X_log - mean) / std\n",
    "\n",
    "# Save normalization stats for inference\n",
    "np.save(\"../DataCollection_Preprocessing/Exported_Paros_Data/mean.npy\", mean)\n",
    "np.save(\"../DataCollection_Preprocessing/Exported_Paros_Data/std.npy\", std)\n",
    "print(\"Saved mean and std to mean.npy and std.npy\")\n",
    "\n",
    "\n",
    "# --- Training with stratified k-fold ---\n",
    "K = 5\n",
    "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "fold_train_accs = []\n",
    "fold_val_accs = []\n",
    "all_val_labels = []\n",
    "all_val_preds = []\n",
    "all_val_probs = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Starting Fold {fold+1}/{K}\")\n",
    "    fold_dir = f\"fold_outputs/fold_{fold+1}\"\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Compute class weights dynamically\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    bias_factor = 3.0\n",
    "    class_weights[1] *= bias_factor\n",
    "\n",
    "    train_dataset = PSD_Dataset(X_train, y_train)\n",
    "    val_dataset = PSD_Dataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = EarthquakeCNN2d(input_shape=X.shape[1:]).to(device)\n",
    "\n",
    "    weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = optim.RAdam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=5, min_delta=1e-4)\n",
    "\n",
    "    num_epochs = 70\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data = data.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Fold {fold+1}, Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    fold_train_accs.append(train_acc)\n",
    "    fold_val_accs.append(val_acc)\n",
    "\n",
    "    model.eval()\n",
    "    val_preds_fold = []\n",
    "    val_labels_fold = []\n",
    "    val_probs_fold = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(data)  # logits\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]  # earthquake class probability\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_preds_fold.extend(predicted.cpu().numpy())\n",
    "            val_labels_fold.extend(targets.cpu().numpy())\n",
    "            val_probs_fold.extend(probs.cpu().numpy())\n",
    "\n",
    "    all_val_labels.extend(val_labels_fold)\n",
    "    all_val_preds.extend(val_preds_fold)\n",
    "    all_val_probs.extend(val_probs_fold)\n",
    "\n",
    "    np.savez(os.path.join(fold_dir, \"data.npz\"),\n",
    "             X_train=X_train, y_train=y_train,\n",
    "             X_val=X_val, y_val=y_val)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(fold_dir, \"CNNmodel.pth\"))\n",
    "\n",
    "    print(f\"\\nClassification report for fold {fold+1}:\")\n",
    "    print(classification_report(val_labels_fold, val_preds_fold, digits=4))\n",
    "\n",
    "print(\"\\n===== Final Cross-Validation Metrics =====\")\n",
    "accuracy = accuracy_score(all_val_labels, all_val_preds)\n",
    "precision = precision_score(all_val_labels, all_val_preds)\n",
    "recall = recall_score(all_val_labels, all_val_preds)\n",
    "f1 = f1_score(all_val_labels, all_val_preds)\n",
    "roc_auc = roc_auc_score(all_val_labels, all_val_probs)\n",
    "cm = confusion_matrix(all_val_labels, all_val_preds)\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")\n",
    "print(f\"ROC AUC:   {roc_auc:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(f\"\\nCross-validation results over {K} folds:\")\n",
    "print(f\"Average Train Accuracy: {np.mean(fold_train_accs):.4f} +/- {np.std(fold_train_accs):.4f}\")\n",
    "print(f\"Average Validation Accuracy: {np.mean(fold_val_accs):.4f} +/- {np.std(fold_val_accs):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2900f9-d713-4608-8eb7-876696e71658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a07fb1-2b43-46dd-8834-351f57289d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125edf8-8970-4e78-a508-ef4595302a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
